{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import random\n",
    "\n",
    "# MediaPipe 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# 웹캠 캡처\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Pose 모델 초기화\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    random_x, random_y = -1, -1  # 초기화 (빨간 점은 처음에 화면에 안 나옴)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # BGR에서 RGB로 변환\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Pose 추적\n",
    "        results = pose.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            # 왼쪽 손목의 랜드마크 위치\n",
    "            left_wrist = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST]\n",
    "            h, w, _ = frame.shape\n",
    "            left_wrist_x, left_wrist_y = int(left_wrist.x * w), int(left_wrist.y * h)\n",
    "\n",
    "            # 빨간 점이 이미 화면에 있으면, 점이 손목과 가까워졌는지 확인\n",
    "            if random_x == -1 and random_y == -1:  # 빨간 점이 없으면 새로 생성\n",
    "                random_x = random.randint(0, w)\n",
    "                random_y = random.randint(0, h)\n",
    "\n",
    "            # 빨간 점 그리기\n",
    "            cv2.circle(frame, (random_x, random_y), 10, (0, 0, 255), -1)\n",
    "\n",
    "            # 손목과 점의 거리가 가까우면 'complete' 표시\n",
    "            if abs(left_wrist_x - random_x) < 30 and abs(left_wrist_y - random_y) < 30:\n",
    "                cv2.putText(frame, \"Complete\", (w // 2 - 100, h // 2), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "                # 'complete'가 표시된 후, 새로운 점을 생성하도록 처리\n",
    "                random_x = random.randint(0, w)\n",
    "                random_y = random.randint(0, h)\n",
    "\n",
    "            # 인체 랜드마크를 파란색으로 그리기\n",
    "            landmark_spec = mp_styles.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=4)\n",
    "            connection_spec = mp_styles.DrawingSpec(color=(255, 0, 0), thickness=2)\n",
    "\n",
    "            # 인체 랜드마크 그리기 (파란색)\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, \n",
    "                landmark_drawing_spec=landmark_spec,\n",
    "                connection_drawing_spec=connection_spec\n",
    "            )\n",
    "\n",
    "        # 화면에 결과 출력\n",
    "        cv2.imshow(\"Pose Estimation\", frame)\n",
    "\n",
    "        # 'q' 키를 누르면 종료\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스쿼트 영상에 모델 적용 및 시각화 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMmodel(\n",
      "  (lstm): LSTM(3, 10, num_layers=2, batch_first=True)\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=3, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from lstm import LSTMmodel\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('mps') if torch.mps.is_available() else torch.device('cpu')\n",
    "\n",
    "# 모델 정의\n",
    "squat_model = 'model/squat_lstm_train.pt'\n",
    "model = LSTMmodel(3,10).to(device)\n",
    "\n",
    "# 저장된 가중치 불러오기\n",
    "model.load_state_dict(torch.load(squat_model, map_location=device, weights_only=True))\n",
    "# model.eval() # 평가 모드로 전환\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Mediapipe 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# 비디오 파일 열기\n",
    "cap = cv2.VideoCapture('squat_data/squat_001.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # 원본 프레임 크기 저장 (결과를 맞추기 위해)\n",
    "    original_frame = frame.copy()\n",
    "\n",
    "    # Mediapipe는 RGB 이미지를 처리하므로 변환\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    # 예상 노드 좌표 (23, 24, 25, 26, 27, 28) 저장\n",
    "    if results.pose_landmarks:\n",
    "        input_data = []\n",
    "        for node_id in [23, 24, 25, 26, 27, 28]:\n",
    "            landmark = results.pose_landmarks.landmark[node_id]\n",
    "            input_data.append([node_id, landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "        # 입력 데이터를 텐서로 변환\n",
    "        input_tensor = torch.tensor([item[1:] for item in input_data], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        # 모델 예측\n",
    "        with torch.no_grad():\n",
    "            predicted_output = model(input_tensor)\n",
    "\n",
    "        # 예측된 좌표 출력 (예측된 출력이 2D 배열일 경우)\n",
    "        predicted_coords = predicted_output.squeeze().cpu().detach().numpy()\n",
    "\n",
    "        # 예측된 좌표가 2D 배열인지 확인\n",
    "        if predicted_coords.ndim == 1:\n",
    "            predicted_coords = predicted_coords.reshape(-1, 3)  # 1D array를 3개의 값으로 변환\n",
    "\n",
    "        # 실제 좌표와 예측된 좌표를 비교하여 빨간 점 그리기\n",
    "        for i, (input_item, predicted_item) in enumerate(zip(input_data, predicted_coords)):\n",
    "            x_real, y_real, z_real = input_item[1], input_item[2], input_item[3]\n",
    "            x_pred, y_pred, z_pred = predicted_item  # 예측된 값이 3개의 좌표값\n",
    "\n",
    "            # 빨간 점을 실제 좌표에 그리기\n",
    "            if abs(x_real - x_pred) > 0.05 or abs(y_real - y_pred) > 0.05:  # 차이가 큰 경우만 빨간 점\n",
    "                cv2.circle(frame, (int(x_pred * frame.shape[1]), int(y_pred * frame.shape[0])), 5, (0, 0, 255), -1)\n",
    "            else:\n",
    "                cv2.circle(frame, (int(x_real * frame.shape[1]), int(y_real * frame.shape[0])), 5, (0, 255, 0), -1)\n",
    "\n",
    "    # 결과 영상 표시\n",
    "    cv2.imshow(\"Squat Pose Prediction\", frame)\n",
    "\n",
    "    # 종료 조건 (q 키)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# 리소스 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 노드별 나타내기가 가능한지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "\n",
    "# Mediapipe 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# 비디오 파일 열기\n",
    "cap = cv2.VideoCapture('squat_data/squat_001.mp4')\n",
    "\n",
    "# 예상된 노드 좌표를 위한 변수 초기화\n",
    "predicted_coords = None\n",
    "random_coords = {23: (-1, -1), 24: (-1, -1), 25: (-1, -1), 26: (-1, -1), 27: (-1, -1), 28: (-1, -1)}\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # 원본 프레임 크기 저장 (결과를 맞추기 위해)\n",
    "    original_frame = frame.copy()\n",
    "\n",
    "    # Mediapipe는 RGB 이미지를 처리하므로 변환\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        input_data = []\n",
    "        for node_id in [23, 24, 25, 26, 27, 28]:\n",
    "            landmark = results.pose_landmarks.landmark[node_id]\n",
    "            input_data.append([node_id, landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "        # 입력 데이터를 텐서로 변환\n",
    "        input_tensor = torch.tensor([item[1:] for item in input_data], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        # 모델 예측\n",
    "        with torch.no_grad():\n",
    "            predicted_output = model(input_tensor)\n",
    "\n",
    "        # 출력 형태 확인\n",
    "        # print(predicted_output.shape)  # 예측된 출력의 shape 확인\n",
    "        predicted_coords = predicted_output.squeeze().cpu().detach().numpy()\n",
    "\n",
    "        # 각 노드에 대해 빨간 점을 그리거나 사라지게 처리\n",
    "        for i, (input_item, predicted_item) in enumerate(zip(input_data, predicted_coords)):\n",
    "            node_id = input_item[0]\n",
    "            x_real, y_real, z_real = input_item[1], input_item[2], input_item[3]\n",
    "            \n",
    "            # 예측된 좌표가 배열로 반환되는지 확인\n",
    "            if isinstance(predicted_item, np.ndarray):\n",
    "                # 예측된 3D 좌표(x, y, z)를 올바르게 할당\n",
    "                x_pred, y_pred, z_pred = predicted_item[0], predicted_item[1], predicted_item[2]\n",
    "            else:\n",
    "                # 예측된 값이 스칼라인 경우 처리\n",
    "                x_pred = predicted_item\n",
    "                y_pred = predicted_item\n",
    "                z_pred = predicted_item\n",
    "\n",
    "            # 화면 좌표로 변환\n",
    "            screen_x = int(x_pred * frame.shape[1])\n",
    "            screen_y = int(y_pred * frame.shape[0])\n",
    "\n",
    "            # 빨간 점이 없으면 새로 생성\n",
    "            if random_coords[node_id] == (-1, -1):\n",
    "                random_coords[node_id] = (screen_x, screen_y)\n",
    "\n",
    "            # 빨간 점 그리기\n",
    "            cv2.circle(frame, random_coords[node_id], 10, (0, 0, 255), -1)\n",
    "\n",
    "            # 실제 좌표와 예측된 좌표 차이를 비교하여 점 사라짐\n",
    "            if abs(x_real - x_pred) < 0.05 and abs(y_real - y_pred) < 0.05:\n",
    "                # 예측된 빨간 점을 실제 좌표에 맞추어 사라지게\n",
    "                random_coords[node_id] = (-1, -1)\n",
    "\n",
    "        # 인체 랜드마크 그리기 (파란색)\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS\n",
    "        )\n",
    "\n",
    "    # 화면에 결과 출력\n",
    "    cv2.imshow(\"Pose Estimation\", frame)\n",
    "\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable numpy.float32 object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m node_id \u001b[38;5;241m=\u001b[39m input_item[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     49\u001b[0m x_real, y_real, z_real \u001b[38;5;241m=\u001b[39m input_item[\u001b[38;5;241m1\u001b[39m], input_item[\u001b[38;5;241m2\u001b[39m], input_item[\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m---> 50\u001b[0m x_pred, y_pred, z_pred \u001b[38;5;241m=\u001b[39m predicted_item  \u001b[38;5;66;03m# 예측된 좌표\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# 화면 좌표로 변환 (예상 좌표)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m screen_x_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(x_pred \u001b[38;5;241m*\u001b[39m frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable numpy.float32 object"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Mediapipe 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# 비디오 파일 열기\n",
    "cap = cv2.VideoCapture('squat_data/squat_001.mp4')\n",
    "\n",
    "# 예상된 노드 좌표를 위한 변수 초기화\n",
    "predicted_coords = None\n",
    "random_coords = {23: (-1, -1), 24: (-1, -1), 25: (-1, -1), 26: (-1, -1), 27: (-1, -1), 28: (-1, -1)}\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # 원본 프레임 크기 저장 (결과를 맞추기 위해)\n",
    "    original_frame = frame.copy()\n",
    "\n",
    "    # Mediapipe는 RGB 이미지를 처리하므로 변환\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        input_data = []\n",
    "        for node_id in [23, 24, 25, 26, 27, 28]:\n",
    "            landmark = results.pose_landmarks.landmark[node_id]\n",
    "            input_data.append([node_id, landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "        # 입력 데이터를 텐서로 변환\n",
    "        input_tensor = torch.tensor([item[1:] for item in input_data], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        # 모델 예측\n",
    "        with torch.no_grad():\n",
    "            predicted_output = model(input_tensor)\n",
    "\n",
    "        # 예측된 좌표 처리 (3D 좌표)\n",
    "        predicted_coords = predicted_output.squeeze().cpu().detach().numpy()\n",
    "\n",
    "        # 각 노드에 대해 빨간 점을 그리거나 사라지게 처리\n",
    "        for i, (input_item, predicted_item) in enumerate(zip(input_data, predicted_coords)):\n",
    "            node_id = input_item[0]\n",
    "            x_real, y_real, z_real = input_item[1], input_item[2], input_item[3]\n",
    "            x_pred, y_pred, z_pred = predicted_item  # 예측된 좌표\n",
    "\n",
    "            # 화면 좌표로 변환 (예상 좌표)\n",
    "            screen_x_pred = int(x_pred * frame.shape[1])\n",
    "            screen_y_pred = int(y_pred * frame.shape[0])\n",
    "\n",
    "            # 빨간 점을 예측된 좌표에 생성\n",
    "            cv2.circle(frame, (screen_x_pred, screen_y_pred), 10, (0, 0, 255), -1)\n",
    "\n",
    "            # 예측된 점과 실제 점을 비교하여 동작 완료 여부 판단\n",
    "            if abs(x_real - x_pred) < 0.05 and abs(y_real - y_pred) < 0.05:\n",
    "                random_coords[node_id] = (-1, -1)\n",
    "\n",
    "        # 인체 랜드마크 그리기 (파란색)\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS\n",
    "        )\n",
    "\n",
    "    # 화면에 결과 출력\n",
    "    cv2.imshow(\"Pose Estimation\", frame)\n",
    "\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고 코드 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_6268\\3651239999.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_squat_1 = torch.load('model_path.pth').to(device)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_path.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 임시 모델 (실제 모델로 교체해야 함)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m model_squat_1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_path.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[0;32m     26\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\kimhi\\anaconda3\\envs\\pytorch-env\\Lib\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\kimhi\\anaconda3\\envs\\pytorch-env\\Lib\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\kimhi\\anaconda3\\envs\\pytorch-env\\Lib\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_path.pth'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Mediapipe 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# 웹캠 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 해상도 설정 (가로, 세로)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# 프레임 속도 설정\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "# 임시 모델 (실제 모델로 교체해야 함)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_squat_1 = torch.load('model_path.pth').to(device)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"웹캠에서 프레임을 읽을 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # Mediapipe는 RGB 이미지를 처리하므로 변환\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "    frame2 = frame.copy()\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb_frame)\n",
    "    black_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "\n",
    "    # 관절 연결 및 포인트 그리기\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            black_frame,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2)\n",
    "        )\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame2,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2)\n",
    "        )\n",
    "\n",
    "    # 입력 데이터 준비\n",
    "    input_data = []\n",
    "    for node_id in [23, 24, 25, 26, 27, 28]:  # 예시로 6개 노드 사용\n",
    "        landmark = results.pose_landmarks.landmark[node_id]\n",
    "        input_data.append([node_id, landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "    # 입력 데이터를 텐서로 변환\n",
    "    input_tensor = torch.tensor([item[1:] for item in input_data], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    # 모델 추론\n",
    "    with torch.no_grad():\n",
    "        predicted_output = model(input_tensor)\n",
    "\n",
    "    # 예측된 좌표 처리 (예측 좌표)\n",
    "    predicted_coords = predicted_output.squeeze().cpu().detach().numpy()\n",
    "\n",
    "    # 예측된 좌표를 실제 웹캠 화면에 표시\n",
    "    for i, (input_item, predicted_item) in enumerate(zip(input_data, predicted_coords)):\n",
    "        node_id = input_item[0]\n",
    "        x_real, y_real, z_real = input_item[1], input_item[2], input_item[3]\n",
    "\n",
    "        # 예측된 좌표는 모델의 출력 (x, y, z)\n",
    "        # predicted_item이 3개의 값을 포함한다고 가정하고 이를 분리\n",
    "        if isinstance(predicted_item, np.ndarray):  # predicted_item이 배열이라면\n",
    "            x_pred, y_pred, z_pred = predicted_item  # 3D 좌표 분리\n",
    "        else:  # 예측된 값이 단일 값인 경우 (비정상적인 출력 방지)\n",
    "            x_pred, y_pred, z_pred = predicted_item, predicted_item, predicted_item\n",
    "\n",
    "        # 화면 좌표로 변환\n",
    "        screen_x_pred = int(x_pred * frame.shape[1])\n",
    "        screen_y_pred = int(y_pred * frame.shape[0])\n",
    "\n",
    "        # 예측된 빨간 점을 그리기\n",
    "        cv2.circle(frame2, (screen_x_pred, screen_y_pred), 10, (0, 0, 255), -1)\n",
    "\n",
    "    # 웹캠 화면과 합성\n",
    "    blended_frame = cv2.addWeighted(frame2, 0.7, black_frame, 1.3, 0)\n",
    "\n",
    "    # 결과를 화면에 표시\n",
    "    cv2.imshow(\"Pose Estimation with Predicted Coordinates\", blended_frame)\n",
    "\n",
    "    # 종료 조건 (q 키)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# 리소스 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "LSTM: Expected input to be 2D or 3D, got 4D instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 모델 추론\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 58\u001b[0m     reconstructed_frame \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# 입력 데이터를 텐서로 변환 (3D로 차원 추가)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([item[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m input_data], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# 차원 3D로 변환\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kimhi\\anaconda3\\envs\\pytorch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kimhi\\anaconda3\\envs\\pytorch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kimhi\\AI_project\\lstm.py:21\u001b[0m, in \u001b[0;36mLSTMmodel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# LSTM 레이어 통과\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     lstm_out, (h_n, c_n) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# lstm_out은 모든 시점의 출력\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# 마지막 시점의 출력만 사용\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     last_hidden_state \u001b[38;5;241m=\u001b[39m lstm_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# (batch_size, hidden_size)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kimhi\\anaconda3\\envs\\pytorch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kimhi\\anaconda3\\envs\\pytorch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kimhi\\anaconda3\\envs\\pytorch-env\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1074\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1073\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m-> 1074\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1075\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM: Expected input to be 2D or 3D, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1076\u001b[0m         )\n\u001b[0;32m   1077\u001b[0m     is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m   1078\u001b[0m     batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: LSTM: Expected input to be 2D or 3D, got 4D instead"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Mediapipe 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# 웹캠 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "# 해상도 설정 (가로, 세로)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)  # 가로 해상도 설정\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)  # 세로 해상도 설정\n",
    "\n",
    "# 프레임 속도 설정\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)  # 30 FPS로 설정\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"웹캠에서 프레임을 읽을 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # Mediapipe는 RGB 이미지를 처리하므로 변환\n",
    "    frame = cv2.resize(frame,(640,480))\n",
    "    frame2 = frame.copy()\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb_frame)\n",
    "    black_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "\n",
    "    # 관절 연결 및 포인트 그리기\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            black_frame, \n",
    "            results.pose_landmarks, \n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2)\n",
    "        )\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame2, \n",
    "            results.pose_landmarks, \n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2)\n",
    "        )\n",
    "    input_tensor = torch.tensor(black_frame[:, :, 1], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)    \n",
    "\n",
    "    # 6개의 랜드마크 좌표 수집\n",
    "    input_data = []\n",
    "    for node_id in [23, 24, 25, 26, 27, 28]:  # 예시로 6개 노드 사용\n",
    "        landmark = results.pose_landmarks.landmark[node_id]\n",
    "        input_data.append([node_id, landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "    # 모델 추론\n",
    "    with torch.no_grad():\n",
    "        reconstructed_frame = model(input_tensor)\n",
    "\n",
    "    # 입력 데이터를 텐서로 변환 (3D로 차원 추가)\n",
    "    input_tensor = torch.tensor([item[1:] for item in input_data], dtype=torch.float32).unsqueeze(0).to(device)  # 차원 3D로 변환\n",
    "\n",
    "\n",
    "\n",
    "    # 조건부 연산: 0.5보다 크면 255, 아니면 0\n",
    "    reconstructed_frame = (reconstructed_frame ==1).int() * 255\n",
    "\n",
    "    # 모델의 출력 변환\n",
    "    output_frame= reconstructed_frame.squeeze().cpu().detach().numpy().astype(np.uint8)\n",
    "    \n",
    "    # output_frame을 3채널로 변환 (BGR)\n",
    "    output_frame_color = cv2.cvtColor(output_frame, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # output_frame_color의 데이터 타입을 frame과 맞춤 (uint8)\n",
    "    output_frame_color = output_frame_color.astype(np.uint8)\n",
    "    \n",
    "    # 웹캠 화면과 합성\n",
    "    output_frame_color = np.zeros((output_frame.shape[0], output_frame.shape[1], 3), dtype=np.uint8)\n",
    "    output_frame_color[:, :, 1] = output_frame # 초록 채널만 활성화\n",
    "    blended_frame_2 = cv2.addWeighted(frame2, 0.7, output_frame_color, 1.3, 0)\n",
    "    \n",
    "    # 결과를 화면에 표시\n",
    "    #cv2.imshow(\"reality_1\", blended_frame_1)\n",
    "    cv2.imshow(\"before\",blended_frame_2)\n",
    "    # 종료 조건 (q 키)\n",
    "\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# 리소스 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "cv2.waitKey(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
